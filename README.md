# üëë MonarchGPT

**"Let the peasants handle the tokens."**

Welcome to **MonarchGPT**, an architecture designed for the aristocracy of Artificial Intelligence. While other models waste their precious GPU cycles attending to every single token like a common democracy, MonarchGPT understands the natural order of things: **Hierarchy**.

We achieve context windows of millions of tokens not by working harder, but by delegating the labor to the lower classes.

## üè∞ The Royal Architecture

Standard Transformers are inefficient because they treat every token as an equal citizen. Disgusting. MonarchGPT implements a strict feudal system to achieve linear complexity on consumer hardware.

### 1. The Worker Layer (The Peasants)
At the bottom, we have the **Workers**. These are small, efficient transformer blocks.
*   **Role:** They toil in the mud of the `local_context` (64 tokens).
*   **Intelligence:** Minimal. They do not see the big picture. They simply predict the next word based on orders from above.
*   **Treatment:** We do not waste global attention on them. They are interchangeable.

### 2. The Manager Layer (The Nobility)
Above the peasantry sits the **Manager**.
*   **Role:** It compresses the chatter of 64 peasants into a single, refined vector using a Convolutional Compressor.
*   **Intelligence:** High. It handles the long-range planning and feeds "Global Context" back down to the workers.
*   **Vector Quantization:** The nobility must speak with decorum. We use VQ (Vector Quantization) to force the managers to snap their thoughts to a discrete codebook, preventing them from mumbling.

### 3. The Skim Memory (The Royal Archives)
This is the crown jewel. How does one rule over a kingdom of 1 Million tokens?
*   **The Problem:** Attending to history is expensive ($O(N^2)$).
*   **The Monarch Solution:** We keep a massive Ring Buffer of past Manager states. We do not train on this buffer (that would require back-propagating through time, which is peasant work).
*   **Top-K Retrieval:** When the Manager speaks, we query the archives. We strictly retrieve only the **Top-K (e.g., 32)** most relevant moments from history.
*   **Result:** Infinite context with constant VRAM usage. We "skim" history rather than memorizing it.

### 4. Monarch Matrices (HBSL)
Dense Linear Layers are wasteful. Why connect everything to everything?
*   We use **Hierarchical Block Shuffle Layers (HBSL)** and Monarch Matrix decompositions.
*   This sparse, structured computation is mathematically elegant and computationally cheap. It is the difference between a brute and a fencer.

---

## üìú How it Works (In plain English)

```python
# The Peasantry (Worker)
# Handles the immediate syntax and grammar.
Worker(Context) + Global_Guidance -> Token

# The Nobility (Manager)
# Looks at a block of 64 peasant tokens, compresses them, 
# and plans the next move.
Manager(Compressed_Block) -> Abstract_Thought

# The Crown (Skim Memory)
# Takes the Manager's current thought, searches a database of 
# 1,000,000+ past thoughts, and retrieves relevant context.
Retrieval(History, Query) -> Long_Term_Memory
```

## üõ†Ô∏è Usage

To summon the court, ensure you have `torch` and `tiktoken` installed. Do not bore me with installation errors.

```bash
pip install torch tiktoken
```

Then, run the training script.

```bash
python monarch_gpt.py
```

### The Modes
*   **Training:** The peasants will begin reading `lovecraft.txt`. You will see the loss drop as they learn their place.
*   **Chat Mode:** Converse with the Monarch.
    *   *Note:* The "Skim Memory" can be set to refresh manually during generation. You tell the peasants when to file their reports to the archives.

## ‚öñÔ∏è License

**The Feudal License:** You may use this code, but you must acknowledge that my architecture is superior to your flat, dense attention mechanisms.

---

*Generated by the Royal Scribe.*
